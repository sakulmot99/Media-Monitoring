# -*- coding: utf-8 -*-
"""total_preprocessing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18eiPhOizoVduwljmj-xIC_tBoU15QI5j

General Cleaning of Scrapped DF
- Input: Scrapped DF
- Output: Clean DF
"""

# df_preprocessing.py
"""
Goal:
Input df structure: url | publisher | title | date | article
Output df structure: url (no duplicates) | publisher | title (clean) | date (harmonized & datetime)
| article (clean) | content (if no article then title) | word_count

Pipeline steps:
1. Drop duplicates based on URL
2. Filter out unwanted URLs
3. Clean text columns (title and article)
4. Create content column
5. Harmonize dates
"""

import pandas as pd
from bs4 import BeautifulSoup as soup
import numpy as np
import logging
import re

# ---------------------------
# Logging setup
# ---------------------------
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# ---------------------------
# 1. Removing duplicates based on URL
# ---------------------------
def remove_duplicates(df: pd.DataFrame, column_name: str = "url") -> pd.DataFrame:
    df = df.copy()
    logging.info(f"Rows before removing duplicates: {len(df)}")
    df = df.drop_duplicates(subset=[column_name], keep='first')
    logging.info(f"Rows after removing duplicates: {len(df)}")
    return df

# ---------------------------
# 2. Removing unwanted URLs
# ---------------------------
def filter_urls(df: pd.DataFrame, keywords: list[str], url_column: str = "url") -> pd.DataFrame:
    """
    Removes rows where the URL contains any of the specified keywords
    """
    df = df.copy()
    pattern = "|".join([re.escape(k) for k in keywords])
    initial_len = len(df)
    df = df[~df[url_column].str.contains(pattern, case=False, na=False)]
    logging.info(f"Filtered URLs: removed {initial_len - len(df)} rows containing keywords {keywords}")
    return df

# ---------------------------
# 3. Cleaning text columns
# ---------------------------
def clean_column(df: pd.DataFrame, column_name: str) -> pd.DataFrame:
    """
    - Removes duplicates based on the column
    - Fills NaNs with 'None'
    - Strips whitespaces and HTML tags
    """
    df = df.copy()
    initial_len = len(df)
    df = df.drop_duplicates(subset=[column_name], keep='first')
    logging.info(f"Removed {initial_len - len(df)} duplicate rows in column '{column_name}'")

    df[column_name] = df[column_name].fillna("None")
    df[column_name] = df[column_name].str.replace(r"\s+", " ", regex=True).str.strip()
    df[column_name] = df[column_name].apply(lambda x: soup(x, "lxml").get_text(separator=" ", strip=True))
    return df

# ---------------------------
# 4. Creating content column
# ---------------------------
def create_content_column(df: pd.DataFrame) -> pd.DataFrame:
    """
    Creates:
    - 'content': uses 'article' if available, otherwise falls back to 'title'
    - 'word_count': number of words in 'content'
    """
    df = df.copy()
    df["content"] = np.where(df["article"] == "None", df["title"], df["article"])
    df["word_count"] = df["content"].apply(lambda x: len(str(x).split()))
    logging.info(f"Content column created with word counts")
    return df

# ---------------------------
# 5. Harmonizing date column
# ---------------------------
def harmonize_dates(date_text):
    """
    Convert various date formats to pandas datetime
    """
    if pd.isna(date_text):
        return pd.NaT
    date_text = str(date_text)
    for pattern in ["Aktualisiert am", "\n", "Uhr"]:
        date_text = date_text.replace(pattern, "")
    # Only keep date part before comma
    date_text = date_text.split(",")[0]
    return pd.to_datetime(date_text, dayfirst=True, errors="coerce")

# ---------------------------
# 6. Full cleaning pipeline
# ---------------------------
def cleaning_pipeline(df: pd.DataFrame) -> pd.DataFrame:
    """
    Runs the full cleaning pipeline:
    1. Remove duplicates
    2. Filter URLs
    3. Clean 'title' and 'article'
    4. Create 'content' column
    5. Harmonize 'date'
    """
    logging.info("Starting cleaning pipeline")
    df = df.copy()
    df = remove_duplicates(df)
    df = filter_urls(df, keywords=["/tests/", "podcast"])
    df = clean_column(df, "title")
    df = clean_column(df, "article")
    df = create_content_column(df)
    df["date"] = df["date"].apply(harmonize_dates)
    logging.info("Cleaning pipeline finished")
    return df