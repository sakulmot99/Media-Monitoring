# -*- coding: utf-8 -*-
"""crawling_main

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fqULsWUuHee9t_g8yy_qtFlwO-A19rZX
"""

from crawling_functions import seed_urls, extract_article_links, select_first, extract_content
from crawling_config import SEEDING_URLS, ARTICLE_IDENTIFIERS, SELECTORS
import pandas as pd

async def crawling_main(df_raw, urls = SEEDING_URLS):

    df_raw = df_raw.copy()
    # 1. Identify all URLs on Seeding domains
    all_links = await seed_urls(urls)

    # 2. Extract article links, remove duplicates against existing_df
    existing_urls = df_raw["url"].tolist() if df_raw is not None else None
    links_articles = extract_article_links(all_links, ARTICLE_IDENTIFIERS, existing_urls)

    # 3. Extract article content
    df = extract_content(links_articles, SELECTORS)

    # 4. Concat
    if df_raw is not None:
        df = pd.concat([df_raw, df], ignore_index=True)

    return df