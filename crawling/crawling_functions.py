# -*- coding: utf-8 -*-
"""crawling_functions

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vwE5L-aBy9oCVlzbav7q1QtiDT1vFhom

### Functions for crawling
"""

# crawler_funcs.py

import pandas as pd
import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import asyncio
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig
from datetime import datetime


# Async URL seeding

async def seed_urls(urls):
    """
    Crawl all pages starting from the seed URLs and collect all links.

    Input: List of URLs
    Output: Dict[str, List[Dict]] = {seed_url: [links]}
    """
    browser_config = BrowserConfig(headless=True, verbose=True)
    run_config = CrawlerRunConfig(
        exclude_external_images=True,
        exclude_all_images=True,
        remove_overlay_elements=True,
        process_iframes=True
    )

    all_links = {}

    async with AsyncWebCrawler(config=browser_config, run_config=run_config) as crawler:
        for url in urls:
            print(f"Crawling URL: {url}")
            result = await crawler.arun(url=url)
            if not result.success:
                print(f"Failed to crawl URL: {url} (status: {result.status_code})")
                continue
            all_links[url] = result.links
            print(f"Found {len(result.links)} links on {url}")

    return all_links


# Extract article links

def extract_article_links(all_links_dict, article_identifiers, existing_df_urls=None):
    """
    Filter links that match article patterns and remove already existing URLs.

    Inputs:
        all_links_dict: Dict from seed_urls
        article_identifiers: Dict[domain] = compiled regex
        existing_df_urls: List of URLs already in your dataframe (optional)

    Output:
        List of new article URLs
    """
    links_articles = []

    for seed_url, links_by_category in all_links_dict.items():
        for category_links in links_by_category.values():
            for link_info in category_links:
                href = link_info.get('href')
                if not href:
                    continue
                domain = urlparse(href).netloc
                pattern = article_identifiers.get(domain)
                if pattern and pattern.search(href):
                    links_articles.append(href)

    if existing_df_urls is not None:
        links_articles = [url for url in links_articles if url not in existing_df_urls]

    print(f"Found {len(links_articles)} new article links")
    return links_articles

# Helper function to handle fallback selectors
def select_first(soup, selectors, multiple=False):
    """
    Try a list of selectors, return first match (or list of elements if multiple=True).
    """
    if isinstance(selectors, str):
        selectors = [selectors]

    for sel in selectors:
        try:
            if multiple:
                elems = soup.select(sel)
                if elems:
                    return elems
            else:
                elem = soup.select_one(sel)
                if elem:
                    return elem
        except Exception:
            continue

    return [] if multiple else None

# Extract article content

def extract_content(article_links, selectors):
    """
    Crawl articles to extract text, titles, and dates.

    Output: pd.DataFrame with columns: url, publisher, title, date, article
    """
    data = []
    print("Extracting Content...")

    for link in article_links:
        try:
            response = requests.get(link, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            domain = urlparse(link).netloc

            # Getting paragraphs
            paragraphs_selector = selectors.get(domain, {}).get("paragraphs")
            paragraphs = select_first(soup, paragraphs_selector, multiple=True)
            article_text = "\n".join([p.get_text() for p in paragraphs]) if paragraphs else None

            # Getting dates
            date_selector = selectors.get(domain, {}).get("date")
            date_element = select_first(soup, date_selector)
            date = date_element.get_text() if date_element else None

            # Getting titles
            title_selector = selectors.get(domain, {}).get("title")
            title_element = select_first(soup, title_selector)
            title = title_element.get_text() if title_element else None

            # Date of Crawling
            date_crawled = datetime.utcnow().isoformat()

            data.append({
                "url": link,
                "publisher": domain,
                "title": title,
                "date": date,
                "article": article_text,
                "date_crawled": date_crawled
            })

        except requests.exceptions.RequestException as e:
            print(f"Error fetching {link}: {e}")
        except Exception as e:
            print(f"Error processing {link}: {e}")

    df = pd.DataFrame(data)
    return df