name: Backend Automation

on:
  schedule:
    - cron: '0 2 * * *'
  workflow_dispatch:

jobs:
  backend_pipeline:
    runs-on: ubuntu-latest

    steps:
    # 1. Checkout repository
    - name: Checkout Repo
      uses: actions/checkout@v3

    # 2. Set up Python
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    # 3. Install dependencies
    - name: Install dependencies
      run: |
        set -e  # Fail if any command fails
        python -m pip install --upgrade pip
        pip install -r requirements/crawling.txt
        playwright install

    # 4. Run Crawling
    - name: Run Crawling
      run: |
        set -e
        python -m crawling/crawling_main

    # 5. Run Preprocessing
    - name: Run Preprocessing
      run: |
        set -e
        python -m preprocessing/preprocessing_main

    # 6. Run Parties Analysis
    - name: Run Parties Analysis
      run: |
        set -e
        python -m parties/parties_main

    # 7. Commit and push updated datasets
    - name: Commit updated datasets
      run: |
        set -e
        git config user.name "github-actions[bot]"
        git config user.email "github-actions[bot]@users.noreply.github.com"

        git add datasets/*.csv
        git commit -m "Automated dataset update [skip ci]" || echo "No changes to commit"
        git push
