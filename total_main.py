# -*- coding: utf-8 -*-
"""total_main

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kE4yWcvcZfIZZeeYpaSJU-9OBebZ1HT4
"""

"""
Input: SEEDING URLS for scrapping
Output: DF_RAW, DF_CLEAN, DF_DISCOURSE, DF_EMBEDDINGS, DF_TOPICS
"""


# --- Environment setup ---
from google.colab import drive
drive.mount("/content/drive")

import os

# Change to the folder where your pipeline files and requirements.txt are
os.chdir("/content/drive/MyDrive/Media Monitoring/Code/Pipeline")

# Confirm current working directory
!pwd
!ls


import sys
sys.path.append("/content/drive/MyDrive/Media Monitoring/Code/Pipeline")

# Install requirements
!pip install -r requirements_main.txt

# Project-wide config
from total_config import RAW_DATA_PATH, CLEAN_DATA_PATH, EMBEDDINGS_DATA_PATH, PARTIES_DATA_PATH, PARTIES_ANALYSIS_PATH

import pandas as pd
import asyncio
import logging
import os

# --- Logging setup ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# --- Step 1: Crawling Pipeline ---


logger.info("Installing Playwright Chromium...")
!playwright install chromium
!playwright install-deps
!bash setup.sh

from crawling_main import crawling_main
from crawling_config import SEEDING_URLS

logger.info("Loading raw data...")
if os.path.exists(RAW_DATA_PATH):
    df_raw = pd.read_csv(RAW_DATA_PATH)
else:
    logger.warning(f"{RAW_DATA_PATH} does not exist. Creating empty DataFrame.")
    df_raw = pd.DataFrame()

logger.info("Running async crawling...")
df_raw = await crawling_main(df_raw, urls=SEEDING_URLS)
logger.info(f"Crawling finished. Total rows: {len(df_raw)}")

print("Step 1: Crawling Pipeline FINISHED")

# --- Step 2: Preprocessing and cleaning ---
from total_preprocessing import cleaning_pipeline
logger.info("Starting cleaning pipeline...")
df_clean = cleaning_pipeline(df_raw)
logger.info(f"Cleaning finished. Total rows: {len(df_clean)}")

print("Step 2: Preprocessing and cleaning FINISHED")


# --- Step 3: Partisan Media Bias Analysis ---
from parties_main import main_parties
from parties_config import PARTY_SYNONYM_DICT, PARTIES, PUBLISHERS
logger.info("Running parties analysis...")
df_parties, df_parties_analysis = main_parties(df_clean, PARTY_SYNONYM_DICT, PARTIES, PUBLISHERS)

logger.info(f"Party analysis finished. Total rows: {len(df_parties)}")
logger.info(f"Aggregated weekly analysis rows: {len(df_parties_analysis)}")

print("Step 3: Partisan Media Bias Analysis FINISHED")

# --- Step 4: Topic Modeling ---
# !pip install -r requirements_topic.txt
#from BERT_main import main_bertopic
#logger.info("Running BERTopic modeling...")
#df_embeddings, df_topics = main_bertopic()
# logger.info(f"BERTopic modeling finished. Embeddings rows: {len(df_embeddings)}, Topics rows: {len(df_topics)}")

#Test
print("**ATTENTION**")
print(len(df_raw))
print(len(df_clean))
print(len(df_parties))
print(len(df_parties_analysis))


# --- Step 5: Save dataframes ---
logger.info("Saving dataframes to files...")
df_raw.to_csv(RAW_DATA_PATH, index=False)
df_clean.to_csv(CLEAN_DATA_PATH, index=False)
df_parties.to_csv(PARTIES_DATA_PATH, index=False)
df_parties_analysis.to_csv(PARTIES_ANALYSIS_PATH, index=False)

#df_embeddings.to_parquet(EMBEDDINGS_DATA_PATH.replace(".csv", "_test.parquet"), index=False)
#df_topics.to_parquet(TOPICS_DATA_PATH.replace(".csv", "_test.parquet"), index=False)
logger.info("All dataframes saved successfully.")